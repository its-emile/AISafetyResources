<html>
  <head>
    <title>AI Safety Resources</title>
    <style>
      .container {
        max-width: 800px;
        margin: 40px auto;
        padding: 20px;
      }
      .center {
        text-align: center;
        margin: 30px 0;
      }
      button {
        padding: 20px 40px;
        font-size: 24px;
        cursor: pointer;
      }
      .links {
        margin-top: 30px;
      }
      .links a {
        display: block;
      }
      a{
        margin: 10px 0;
        color: #0066cc;
        text-decoration: none;
      }
      a:hover {
        text-decoration: underline;
      }
    </style>
  </head>
  <body>
    <div class="container">
      If you are joining or growing in the field of AI safety, here are some resources you may want to review regarding risks, 
      mitigations, and also to understand "why" within generative AI architectures. Also check out aisafety.com's list 
      of <a href="https://www.aisafety.com/courses">courses</a>, <a href="https://www.aisafety.com/events-and-training">events/trainings</a>, 
      and <a href="https://www.aisafety.com/communities">communities</a>.
      <div class="center">
        <button onclick="pickRandom()">Pick one for me</button>
      </div>
      <div class="links" id="linkList"></div>
    </div>
    <script>
      const resources = [
        { title: "International AI Safety Report (2025)", url: "https://arxiv.org/abs/2501.17805" },
        { title: "Robert Miles: Fundamentals of why AI is not under control", url: "https://www.youtube.com/@RobertMilesAI/videos" },
        { title: "Ajeya Cotra: Countermeasures Matter to Prevent AI Takeover", url: "https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to" },
        { title: "Mustafa Suleyman: The Coming Wave - unprecedented risks of AI & adjacent tech + how we might contain them", url: "https://www.penguinrandomhouse.com/books/722674/the-coming-wave-by-mustafa-suleyman-with-michael-bhaskar/" },
        { title: "AI Risk Taxonomy/Categorization (2024)", url: "https://arxiv.org/html/2406.17864v1" },
        { title: "AI Safety: THE BASICS", url: "https://course.bluedot.org/ai-impact&unit=1" },
        { title: "BlueDot Fast Track: Alignment Courses", url: "https://course.aisafetyfundamentals.com/alignment-fast-track" },
        { title: "BlueDot Fast Track: Governance Courses", url: "https://course.aisafetyfundamentals.com/governance-fast-track" },
        { title: "Eval dashboards: Lakera model risk index", url: "https://www.lakera.ai/ai-model-risk-index"}, 
        { title: "Eval dashboards: EnkryptAI LLM safety leaderboard", url: "https://www.enkryptai.com/llm-safety-leaderboard"},
        { title: "Eval dashboards: LLM trustworthy leaderboard on Huggingface", url: "https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard"},
        { title: "Eval dashboards: Compl-AI.org evaluations", url: "https://compl-ai.org/evaluations/"},
        { title: "Eval dasbhoards: Arcadia dashboard of Inspect evals (disclosure: I'm a contributor)", url: "bit.ly/inspect-dashboard-arcadia"},
        { title: "NIST AI Risk Management Framework Playbook", url: "https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook" },
        { title: "NIST GenAI Risk profiles (taxonomy)", url:"https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf" },
        { title: "NIST Secure software development framework: profile for Generative AI applications", url: "https://csrc.nist.gov/pubs/sp/800/218/a/final" },
        { title: "MITRE ATLAS Matrix", url: "https://atlas.mitre.org/matrices/ATLAS" },
        { title: "AI Alignment Forum - Best of 2023", url: "https://www.alignmentforum.org/" },
        { title: "AI Alignment Forum - Best of N Attacks", url: "https://www.alignmentforum.org/posts/oq5CtbsCncctPWkTn/best-of-n-jailbreaking" },
        { title: "AI Alignment Forum - Best of N Mitigations", url: "https://www.alignmentforum.org/posts/5MyB8k8hSJRfa3udi/defense-against-the-dark-prompts-mitigating-best-of-n" },
        { title: "OWASP: GenAI Top 10 Risks", url: "https://genai.owasp.org/llm-top-10/" },
        { title: "OWASP: GenAI Checklist", url: "https://genai.owasp.org/resource/llm-applications-cybersecurity-and-governance-checklist-english/" },
        { title: "OWASP: GenAI Solution Landscape", url: "https://genai.owasp.org/ai-security-solutions-landscape/" },
        { title: "OWASP: GenAI threats & mitigations for agentic/multi-agentic AI systems (feb 2025)", url: "https://genaisecurityproject.com/resource/agentic-ai-threats-and-mitigations/" },
        { title: "OWASP: Securing agentic applications guide 1.0 (April 2025, disclosure: I am a contributor)", url: "https://genai.owasp.org/resource/securing-agentic-applications-guide-1-0/" },
        { title: "Anthropic: 2025 recommended safety research directions", url: "https://alignment.anthropic.com/2025/recommended-directions/" },
        { title: "Anytime you rely on an LLM to enforce something important, you will fail - a primer on agents, July 2025", url: "https://lethain.com/what-can-agents-do/" }
      ];

      function pickRandom() {
        const randomResource = resources[Math.floor(Math.random() * resources.length)];
        window.location = randomResource.url;
      }

      function displayLinks() {
        const linkList = document.getElementById('linkList');
        resources.forEach(resource => {
          const link = document.createElement('a');
          link.href = resource.url;
          link.textContent = resource.title;
          linkList.appendChild(link);
        });
      }

      displayLinks();
    </script>
  </body>
</html>
